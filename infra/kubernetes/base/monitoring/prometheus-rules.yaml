apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: lomash-wood-prometheus-rules
  namespace: lomash-wood
  labels:
    app.kubernetes.io/name: lomash-wood-prometheus-rules
    app.kubernetes.io/part-of: lomash-wood-platform
    release: prometheus
spec:
  groups:

    - name: lomash-wood.recording-rules
      interval: 30s
      rules:
        - record: job:http_requests_total:rate5m
          expr: sum(rate(http_requests_total[5m])) by (job, service, method, status_code)

        - record: job:http_request_duration_seconds:p50
          expr: histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job, service))

        - record: job:http_request_duration_seconds:p95
          expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job, service))

        - record: job:http_request_duration_seconds:p99
          expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job, service))

        - record: job:http_error_rate:rate5m
          expr: |
            sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (job, service)
            /
            sum(rate(http_requests_total[5m])) by (job, service)

        - record: job:process_cpu_usage:avg5m
          expr: avg(rate(process_cpu_seconds_total[5m])) by (job, service)

        - record: job:process_memory_usage_bytes:avg
          expr: avg(process_resident_memory_bytes) by (job, service)

        - record: job:nodejs_eventloop_lag_seconds:p99
          expr: histogram_quantile(0.99, sum(rate(nodejs_eventloop_lag_seconds_bucket[5m])) by (le, job, service))

    - name: lomash-wood.availability
      rules:
        - alert: ServiceDown
          expr: up{namespace="lomash-wood"} == 0
          for: 1m
          labels:
            severity: critical
            team: platform
            namespace: lomash-wood
          annotations:
            summary: "Service {{ $labels.service }} is down"
            description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has been unreachable for more than 1 minute."
            runbook: "https://docs.lomashwood.co.uk/runbooks/outage"

        - alert: PodNotReady
          expr: |
            kube_pod_status_ready{namespace="lomash-wood", condition="true"} == 0
          for: 2m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Pod {{ $labels.pod }} is not ready"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has not been ready for 2 minutes."
            runbook: "https://docs.lomashwood.co.uk/runbooks/outage"

        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total{namespace="lomash-wood"}[15m]) * 60 * 15 > 3
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Pod {{ $labels.pod }} is crash looping"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted more than 3 times in the last 15 minutes."
            runbook: "https://docs.lomashwood.co.uk/runbooks/outage"

        - alert: DeploymentReplicasMismatch
          expr: |
            kube_deployment_spec_replicas{namespace="lomash-wood"}
            != kube_deployment_status_available_replicas{namespace="lomash-wood"}
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Deployment {{ $labels.deployment }} has unavailable replicas"
            description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} fewer replicas than desired."

    - name: lomash-wood.latency
      rules:
        - alert: HighP99Latency
          expr: job:http_request_duration_seconds:p99 > 3
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High P99 latency on {{ $labels.service }}"
            description: "Service {{ $labels.service }} P99 latency is {{ $value | humanizeDuration }}, exceeding the 3 second SLA threshold."
            runbook: "https://docs.lomashwood.co.uk/runbooks/high-latency"

        - alert: CriticalP99Latency
          expr: job:http_request_duration_seconds:p99 > 10
          for: 2m
          labels:
            severity: critical
            team: backend
          annotations:
            summary: "Critical P99 latency on {{ $labels.service }}"
            description: "Service {{ $labels.service }} P99 latency is {{ $value | humanizeDuration }}, critically exceeding thresholds."
            runbook: "https://docs.lomashwood.co.uk/runbooks/high-latency"

        - alert: HighP95Latency
          expr: job:http_request_duration_seconds:p95 > 1
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "Elevated P95 latency on {{ $labels.service }}"
            description: "Service {{ $labels.service }} P95 latency is {{ $value | humanizeDuration }}."

    - name: lomash-wood.error-rates
      rules:
        - alert: HighErrorRate
          expr: job:http_error_rate:rate5m > 0.05
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High error rate on {{ $labels.service }}"
            description: "Service {{ $labels.service }} is returning errors at {{ $value | humanizePercentage }} over the last 5 minutes."
            runbook: "https://docs.lomashwood.co.uk/runbooks/high-error-rate"

        - alert: CriticalErrorRate
          expr: job:http_error_rate:rate5m > 0.20
          for: 2m
          labels:
            severity: critical
            team: backend
          annotations:
            summary: "Critical error rate on {{ $labels.service }}"
            description: "Service {{ $labels.service }} error rate is {{ $value | humanizePercentage }} â€” immediate action required."
            runbook: "https://docs.lomashwood.co.uk/runbooks/high-error-rate"

    - name: lomash-wood.resources
      rules:
        - alert: HighCPUUsage
          expr: |
            rate(container_cpu_usage_seconds_total{namespace="lomash-wood", container!=""}[5m])
            / on(pod, container)
            kube_pod_container_resource_limits{namespace="lomash-wood", resource="cpu"}
            > 0.85
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "High CPU usage on {{ $labels.pod }}/{{ $labels.container }}"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its CPU limit."

        - alert: HighMemoryUsage
          expr: |
            container_memory_working_set_bytes{namespace="lomash-wood", container!=""}
            / on(pod, container)
            kube_pod_container_resource_limits{namespace="lomash-wood", resource="memory"}
            > 0.85
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "High memory usage on {{ $labels.pod }}/{{ $labels.container }}"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit."

        - alert: MemoryNearOOMKill
          expr: |
            container_memory_working_set_bytes{namespace="lomash-wood", container!=""}
            / on(pod, container)
            kube_pod_container_resource_limits{namespace="lomash-wood", resource="memory"}
            > 0.95
          for: 2m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Container {{ $labels.container }} near OOM kill"
            description: "Container {{ $labels.container }} in pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit and may be OOM killed."

        - alert: HPAMaxReplicasReached
          expr: |
            kube_horizontalpodautoscaler_status_current_replicas{namespace="lomash-wood"}
            >= kube_horizontalpodautoscaler_spec_max_replicas{namespace="lomash-wood"}
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "HPA {{ $labels.horizontalpodautoscaler }} at max replicas"
            description: "HPA {{ $labels.horizontalpodautoscaler }} has reached its maximum replica count. Consider increasing maxReplicas."

    - name: lomash-wood.nodejs
      rules:
        - alert: HighEventLoopLag
          expr: job:nodejs_eventloop_lag_seconds:p99 > 0.5
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High Node.js event loop lag on {{ $labels.service }}"
            description: "Service {{ $labels.service }} has a P99 event loop lag of {{ $value | humanizeDuration }}, indicating CPU starvation or blocking I/O."

        - alert: HighOpenFileDescriptors
          expr: |
            process_open_fds{namespace="lomash-wood"}
            / process_max_fds{namespace="lomash-wood"}
            > 0.80
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High open file descriptors on {{ $labels.service }}"
            description: "Service {{ $labels.service }} is using {{ $value | humanizePercentage }} of its file descriptor limit."

        - alert: ActiveHandlesLeak
          expr: nodejs_active_handles_total{namespace="lomash-wood"} > 5000
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "Possible handle leak on {{ $labels.service }}"
            description: "Service {{ $labels.service }} has {{ $value }} active handles, which may indicate a resource leak."

    - name: lomash-wood.database
      rules:
        - alert: PostgresConnectionsNearLimit
          expr: |
            pg_stat_activity_count{namespace="lomash-wood"}
            / pg_settings_max_connections{namespace="lomash-wood"}
            > 0.80
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "PostgreSQL connections near limit"
            description: "PostgreSQL is using {{ $value | humanizePercentage }} of max connections."
            runbook: "https://docs.lomashwood.co.uk/runbooks/database-failure"

        - alert: PostgresReplicationLag
          expr: pg_replication_lag{namespace="lomash-wood"} > 30
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "PostgreSQL replication lag is {{ $value }}s"
            description: "PostgreSQL replica is {{ $value }} seconds behind the primary."
            runbook: "https://docs.lomashwood.co.uk/runbooks/database-failure"

        - alert: PrismaPoolExhausted
          expr: prisma_pool_connections_busy{namespace="lomash-wood"} >= prisma_pool_connections_limit{namespace="lomash-wood"}
          for: 3m
          labels:
            severity: critical
            team: backend
          annotations:
            summary: "Prisma connection pool exhausted on {{ $labels.service }}"
            description: "Service {{ $labels.service }} has exhausted its Prisma connection pool. Requests may be queuing or failing."

    - name: lomash-wood.redis
      rules:
        - alert: RedisDown
          expr: redis_up{namespace="lomash-wood"} == 0
          for: 1m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Redis is down"
            description: "Redis instance is unreachable. Caching, session management, and rate limiting are degraded."
            runbook: "https://docs.lomashwood.co.uk/runbooks/database-failure"

        - alert: RedisMemoryNearLimit
          expr: |
            redis_memory_used_bytes{namespace="lomash-wood"}
            / redis_memory_max_bytes{namespace="lomash-wood"}
            > 0.85
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Redis memory usage at {{ $value | humanizePercentage }}"
            description: "Redis is using {{ $value | humanizePercentage }} of its configured max memory. Risk of key eviction."

        - alert: RedisHighKeyEvictionRate
          expr: rate(redis_evicted_keys_total{namespace="lomash-wood"}[5m]) > 10
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Redis is evicting keys at {{ $value }}/s"
            description: "Redis is evicting {{ $value }} keys per second. Cache hit rate may be degrading."

    - name: lomash-wood.payments
      rules:
        - alert: StripeWebhookFailureRate
          expr: |
            rate(stripe_webhook_failures_total{namespace="lomash-wood"}[5m])
            / rate(stripe_webhook_total{namespace="lomash-wood"}[5m])
            > 0.05
          for: 5m
          labels:
            severity: critical
            team: payments
          annotations:
            summary: "High Stripe webhook failure rate"
            description: "Stripe webhook failure rate is {{ $value | humanizePercentage }}. Payment status updates may be delayed."
            runbook: "https://docs.lomashwood.co.uk/runbooks/payment-incident"

        - alert: PaymentIntentFailureRate
          expr: |
            rate(payment_intent_failures_total{namespace="lomash-wood"}[10m])
            / rate(payment_intent_total{namespace="lomash-wood"}[10m])
            > 0.10
          for: 5m
          labels:
            severity: critical
            team: payments
          annotations:
            summary: "High payment intent failure rate"
            description: "Payment intent failure rate is {{ $value | humanizePercentage }}."
            runbook: "https://docs.lomashwood.co.uk/runbooks/payment-incident"

        - alert: AbandonedOrdersHigh
          expr: order_abandoned_total{namespace="lomash-wood"} > 50
          for: 15m
          labels:
            severity: warning
            team: payments
          annotations:
            summary: "High abandoned orders count"
            description: "There are {{ $value }} abandoned orders in the last 15 minutes."

    - name: lomash-wood.auth
      rules:
        - alert: HighAuthFailureRate
          expr: |
            rate(auth_login_failures_total{namespace="lomash-wood"}[5m]) > 20
          for: 5m
          labels:
            severity: warning
            team: security
          annotations:
            summary: "High authentication failure rate"
            description: "Auth service is seeing {{ $value }} login failures per second. Possible brute force attempt."
            runbook: "https://docs.lomashwood.co.uk/runbooks/auth-failure"

        - alert: TokenBlacklistSizeLarge
          expr: auth_token_blacklist_size{namespace="lomash-wood"} > 100000
          for: 30m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "JWT token blacklist is large ({{ $value }} entries)"
            description: "The token blacklist has grown to {{ $value }} entries. Consider running the cleanup job."

    - name: lomash-wood.appointments
      rules:
        - alert: AppointmentBookingFailureRate
          expr: |
            rate(appointment_booking_failures_total{namespace="lomash-wood"}[10m])
            / rate(appointment_booking_attempts_total{namespace="lomash-wood"}[10m])
            > 0.10
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High appointment booking failure rate"
            description: "Appointment booking failure rate is {{ $value | humanizePercentage }}."

        - alert: AppointmentReminderBacklog
          expr: appointment_reminder_queue_depth{namespace="lomash-wood"} > 500
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "Appointment reminder queue backlog ({{ $value }} items)"
            description: "There are {{ $value }} pending appointment reminders. Reminder delivery may be delayed."

    - name: lomash-wood.notifications
      rules:
        - alert: EmailDeliveryFailureRate
          expr: |
            rate(email_delivery_failures_total{namespace="lomash-wood"}[10m])
            / rate(email_delivery_attempts_total{namespace="lomash-wood"}[10m])
            > 0.05
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High email delivery failure rate"
            description: "Email delivery failure rate is {{ $value | humanizePercentage }}."

        - alert: NotificationQueueDepthHigh
          expr: notification_queue_depth{namespace="lomash-wood"} > 1000
          for: 10m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "Notification queue backlog is {{ $value }}"
            description: "The notification service has a queue backlog of {{ $value }} messages. Processing may be delayed."

    - name: lomash-wood.content
      rules:
        - alert: MediaUploadFailureRate
          expr: |
            rate(media_upload_failures_total{namespace="lomash-wood"}[10m])
            / rate(media_upload_attempts_total{namespace="lomash-wood"}[10m])
            > 0.10
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High media upload failure rate"
            description: "Media upload failure rate is {{ $value | humanizePercentage }}."

        - alert: S3UploadLatencyHigh
          expr: |
            histogram_quantile(0.95, rate(s3_upload_duration_seconds_bucket{namespace="lomash-wood"}[5m])) > 10
          for: 5m
          labels:
            severity: warning
            team: backend
          annotations:
            summary: "High S3 upload latency"
            description: "S3 upload P95 latency is {{ $value | humanizeDuration }}, which may impact the CMS experience."